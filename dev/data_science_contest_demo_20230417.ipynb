{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowpark デモ\n",
    "\n",
    "やっぱり実際動いているものを見るのが一番わかりやすいですよね！\n",
    "というわけで、Snowpark でデータを操作してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# おしながき\n",
    "\n",
    "- [Snowpark の基本](#snowpark-の基本)\n",
    "    - [接続設定](接続設定)\n",
    "    - [DataFrame の操作](DataFrame-の操作)\n",
    "    - [UDF とストアドプロシージャ](UDF-とストアドプロシージャ)\n",
    "- [機械学習](機械学習)\n",
    "    - [学習とモデルの保存](学習とモデルの保存)\n",
    "    - [推論](推論)\n",
    "- [提供データを Snowpark で触ってみよう](提供データを-Snowpark-で触ってみよう)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowpark の基本\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接続設定\n",
    "ますはセッションを作成しましょう。\n",
    "（Snowsight などの Web UI や Snowflake クライアントで接続するときに作成されるセッションと同じ概念です）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行環境へのパスを通す（念のため）\n",
    "current_path = '/home/dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# notebook フォルダがパスに入っていないので追加\n",
    "sys.path.append(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "from config import connection_parameters\n",
    "\n",
    "# config.py にはこのようなものが書いてあります：\n",
    "# connection_parameters = {\n",
    "#    \"account\": \"xx00000.ap-northeast-1.aws\",\n",
    "#     \"user\": \"<you_username>\",\n",
    "#     \"private_key\": pkb,\n",
    "#     \"role\": \"<your_role>\",\n",
    "#     \"database\": \"<your_database>\",\n",
    "#     \"schema\": \"<your_schema>\",\n",
    "#     \"warehouse\": \"<your_warehouse>\",\n",
    "# }\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "試しに、現在、使用しているウェアハウス、データベース、スキーマを確認してみましょう。\n",
    "\n",
    "クエリを実行するには `sql(<クエリ>).collect()` です。`collect()` や `show()` を呼ぶまで実行されないので注意（遅延評価）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.sql('select current_warehouse(), current_database(), current_schema()').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show() では表形式になる\n",
    "session.sql('select current_warehouse(), current_database(), current_schema()').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame の操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それではさっそく Snowpark で Dataframe の操作をしてみましょう。\n",
    "\n",
    "今回、提供いただいているデータの中から、マインディアさんのデータを見ていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こんな感じのデータです。\n",
    "\n",
    "| カラム名 | 型 | 説明 | \n",
    "| :- | :- | :- | \n",
    "| AGE | VARCHAR(16777216) | 年代（20～49代） |\n",
    "| AMOUNT_TO_RATIO | FLOAT | 推定購入値（※）|\n",
    "| GENDER | VARCHAR(16777216) | 性別（男性/女性) |\n",
    "| PURCHASE_AREA | VARCHAR(16777216) | 地域<br>（東京23区・大阪市・名古屋市、区ごと） |\n",
    "| PURCHASE_DATE | DATE  | 購入日 |\n",
    "\n",
    "※ PURCHASE_DATA、GENDER、AGE、PURCHASE_AREA でグルーピングしたとき、全体平均売上（1とする）に対するグループの売り上げ比率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.table('MINEDIA_CONTEST.CONTEST.PURCHASE_DELIVERY')\n",
    "df.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、、、Pandas Dataframe で表示したほうが、綺麗な感じになったりしますが、今回は Snowpark のお話なので、しません！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(df.limit(10).collect())\n",
    "# df.limit(10).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基礎集計を出すこともできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、よくある DataFrame の操作を行ってみましょう。\n",
    "\n",
    "フィルタリング（SQL でいう Where 句）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "\n",
    "df.where(\n",
    "    F.col('AMOUNT_TO_RATIO') > 10\n",
    ").select(\n",
    "    F.col('PURCHASE_AREA'), F.col('PURCHASE_DATE'), F.col('AMOUNT_TO_RATIO')\n",
    ").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "グルーピング（group by）、集計、ソート（order by）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.group_by(\n",
    "    F.col('PURCHASE_AREA')  # 地域ごとに\n",
    ").agg(\n",
    "    F.avg(F.col('AMOUNT_TO_RATIO')).as_('AMOUNT_TO_RATIO_AVG'),  # 推定購入値の平均\n",
    "    F.max(F.col('AMOUNT_TO_RATIO')).as_('AMOUNT_TO_RATIO_MAX'),  # 推定購入値の最大\n",
    "    F.min(F.col('AMOUNT_TO_RATIO')).as_('AMOUNT_TO_RATIO_MIN')  # 推定購入値の最小\n",
    ").sort(\n",
    "    F.col('AMOUNT_TO_RATIO_AVG'),  # 推定購入値平均の降順で並べ替え\n",
    "    ascending = False\n",
    ").limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結合（JOIN）のために、もう1つテーブルを使います。\n",
    "truestar さんのカレンダーデータを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calender = session.table(\"PODB__JAPANESE_OPEN_DATA_SAMPLE_DATASETS.CALENDAR.E_JAPAN_CALENDAR\")\n",
    "df_calender.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば、カレンダー情報を日付でくっつけるなら、このようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.join(  # カレンダー情報をinner joinで付与\n",
    "    df_calender,\n",
    "    df.PURCHASE_DATE == df_calender.DATE,\n",
    "    join_type = 'inner'\n",
    ").select(\n",
    "    F.col('PURCHASE_DATE'), F.col('PURCHASE_AREA'), F.col('AMOUNT_TO_RATIO'), F.col('WEEKDAY'), F.col('HOLIDAY_NAME')\n",
    ").sort(\n",
    "    F.col('AMOUNT_TO_RATIO'),  # 推定購入値の降順で並べ替え\n",
    "    ascending = False\n",
    ").limit(10).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "。。。というように、Snowpark で DataFrame を扱うことができます。\n",
    "意外と簡単、と思われた方も多いのではないでしょうか？（そうであってほしい）\n",
    "\n",
    "なお、Pandas を使われている方は、こちらの記事を読んで Pandas API に対応する Snowpark API を捜すと、わかりやすいかもしれません。\n",
    "\n",
    "https://qiita.com/takada_tf/items/62f0337d80508631db57#nunique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF とストアドプロシージャ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて UDF と ストアドプロシージャを作ってみましょう。\n",
    "\n",
    "UDF（User Defined Function）は、データベースシステムに組み込まれているシステム関数（`count()`、 `sqrt()` のような関数）に対して、ユーザーが定義する関数のことです。\n",
    "ここでは、文字の全角・半角を正規化する関数を作ってみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creating stage on shared database 'SNOWFLAKE_SAMPLE_DATA' is not allowed. なので\n",
    "session.sql(f\"use database {connection_parameters['database']}\").collect()\n",
    "print(session.sql('select current_warehouse(), current_database(), current_schema()').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import udf\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "@udf(\n",
    "    name='unicode_nfkc_udf',\n",
    "    is_permanent=True,\n",
    "    stage_location='@~/sample_udf_stage',\n",
    "    replace=True\n",
    ")\n",
    "def unicode_nfkc_udf(x: str) -> str:\n",
    "    return unicodedata.normalize('NFKC', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "早速使ってみましょう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show() では表形式になる\n",
    "session.sql(\n",
    "    \"select unicode_nfkc_udf('全角半角ｶﾅ混じり　表記ゆれてるＹＯ!')\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次はストアドプロシージャを作ってみましょう。ストアドプロシージャも UDF と同様、コードを書くことで SQL を拡張できちゃうやつです。 管理操作を実行させたい、などの場合はこちらを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import sproc\n",
    "\n",
    "\n",
    "@sproc(\n",
    "    name=\"count_sproc\",\n",
    "    packages=['snowflake-snowpark-python'],\n",
    "    is_permanent=True,\n",
    "    stage_location=\"@~/sample_sproc_stage\",\n",
    "    replace=True\n",
    ")\n",
    "def count_sproc(session: Session, table: str, column: str) -> int:\n",
    "    df = session.table(table)\n",
    "    # ストアドプロシージャ内では SELECT、 UPDATE、 CREATE などのデータベース操作を実行可能だったり\n",
    "    return df.agg(F.count(F.col(column))).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらも使ってみましょう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    count_sproc(\n",
    "        session,\n",
    "        'MINEDIA_CONTEST.CONTEST.PURCHASE_DELIVERY',\n",
    "        'AMOUNT_TO_RATIO'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちゃんと `df.describe()` とあってますね！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでで Snowpark の3大要素を見てきました。次はいよいよ機械学習です！\n",
    "\n",
    "先ほどのマインディアさんのデータを使って、推定購入値を予測するモデルを作ってみましょう。\n",
    "\n",
    "今回は機械学習モデルを動かす例として、データの前処理・パラメータチューニングなどは割愛させていただきます。もし機械学習を用いたソリューションを考えられるなら、探索的データ分析をしっかり行ってからのほうが、よきモデルになると思います！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.window import Window\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "\n",
    "\n",
    "table_name = 'MINEDIA_CONTEST.CONTEST.PURCHASE_DELIVERY'\n",
    "\n",
    "\n",
    "# 移動平均をとるためにウインドウ関数の条件を作っておく\n",
    "# rows_between() で発行されるクエリが ROW BETWEEN <start> FOLLOWING  AND <end> なので合わせる\n",
    "window_3days = Window.order_by(F.col('PURCHASE_DATE').desc()).rows_between(Window.CURRENT_ROW, 2)\n",
    "window_1week = Window.order_by(F.col('PURCHASE_DATE').desc()).rows_between(Window.CURRENT_ROW, 6)\n",
    "\n",
    "df_ml = session.table(table_name)\n",
    "\n",
    "df_ml = df_ml.where(\n",
    "    (F.col('PURCHASE_AREA') == '港区')\n",
    "    & (F.col('AGE') == '30代')\n",
    "    & (F.col('GENDER') == '女性')\n",
    ").select(\n",
    "    F.col('PURCHASE_DATE'),\n",
    "    F.col('AMOUNT_TO_RATIO'),\n",
    "    F.avg(F.col('AMOUNT_TO_RATIO')).over(window_3days).name('MOVING_3DAY_AMOUNT_TO_RATIO_AVG'),\n",
    "    F.avg(F.col('AMOUNT_TO_RATIO')).over(window_1week).name('MOVING_1WEEK_AMOUNT_TO_RATIO_AVG')\n",
    ").order_by(\n",
    "    F.col('PURCHASE_DATE').asc()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "というわけで、港区でフードデリバリを利用された30代女性のデータを抽出してみました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "時系列で推定購入値を見ると、こんな感じ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark.functions as F\n",
    "import pandas as pb\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "pd.DataFrame(\n",
    "    df_ml.select(\n",
    "        F.col('PURCHASE_DATE'),\n",
    "        F.col('AMOUNT_TO_RATIO')\n",
    "    ).collect()\n",
    ").set_index(\n",
    "    'PURCHASE_DATE'\n",
    ").plot(\n",
    "    figsize=(15,5)\n",
    ")\n",
    "# もしくは\n",
    "# df.select(\n",
    "#     F.col('PURCHASE_DATE'),\n",
    "#     F.col('AMOUNT_TO_RATIO')\n",
    "# ).to_pandas().set_index(\n",
    "#     'PURCHASE_DATE'\n",
    "# ).plot(\n",
    "#     figsize=(15,5)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前にデータセットをトレーニング用/評価用に分けます。\n",
    "今回は、\n",
    "\n",
    "- トレーニング用： 2020-01-01 ～ 2020-09-30\n",
    "- 評価用：2020-10-01～\n",
    "\n",
    "と分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table_name = 'DF_ML_TRAIN'\n",
    "test_table_name = 'DF_ML_TEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark.types as T\n",
    "\n",
    "\n",
    "df_ml_train = df_ml.where(\n",
    "    (F.col('PURCHASE_DATE') >= '2020-01-01')\n",
    "    & (F.col('PURCHASE_DATE') <= '2020-09-30')\n",
    ")\n",
    "df_ml_test = df_ml.where(\n",
    "    F.col('PURCHASE_DATE') >= '2020-10-01'\n",
    ")\n",
    "\n",
    "df_ml_train.write.mode(\"overwrite\").save_as_table(train_table_name, table_type=\"temporary\")\n",
    "df_ml_test.write.mode(\"overwrite\").save_as_table(test_table_name, table_type=\"temporary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    df_ml_train.select(\n",
    "        F.col('PURCHASE_DATE'),\n",
    "        F.col('AMOUNT_TO_RATIO')\n",
    "    ).collect()\n",
    ").set_index(\n",
    "    'PURCHASE_DATE'\n",
    ").plot(\n",
    "    figsize=(15,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    df_ml_test.select(\n",
    "        F.col('PURCHASE_DATE'),\n",
    "        F.col('AMOUNT_TO_RATIO')\n",
    "    ).collect()\n",
    ").set_index(\n",
    "    'PURCHASE_DATE'\n",
    ").plot(\n",
    "    figsize=(15,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習とモデルの保存\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を実行してみましょう。\n",
    "今回は、ストアドプロシージャで学習の実行と学習済みモデルの保存を行います。\n",
    "\n",
    "学習を実行する際、集計や変換などの処理より大きなメモリが必要な場合があります。その場合は Snowpark 最適化インスタンスを使うとよいでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必要なパッケージがいくつかあるので、インストールします。[`install_packages`](./install_packages) で使用するパッケージをインストールできます。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook 上でインストール時コマンドを実行することもできます\n",
    "# !conda install numpy==1.23.5 pandas==1.5.3 cachetools==4.2.2 dill==0.3.6 matplotlib==3.7.1 seaborn==0.12.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル保存用のステージを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stage_name = 'model_stage'\n",
    "session.sql(f'create or replace stage \"{model_stage_name}\";').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import snowflake.snowpark\n",
    "from snowflake.snowpark import FileOperation\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.types as T\n",
    "\n",
    "import io\n",
    "import dill\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def save_file(\n",
    "            session: snowflake.snowpark.Session, \n",
    "            model, \n",
    "            path: str\n",
    "        ) -> str:\n",
    "    ''' model を session で接続している環境の path に保存する'''\n",
    "    input_stream = io.BytesIO()\n",
    "    dill.dump(model, input_stream)\n",
    "    try:\n",
    "        session._conn._cursor.upload_stream(input_stream, path)\n",
    "        msg = \"successfully created file: \" + path\n",
    "    except Exception as e:\n",
    "        msg = f'upload stream no exists. ({e})'\n",
    "    \n",
    "    return msg\n",
    "\n",
    "\n",
    "def train_xgboost_model(\n",
    "        session: Session, \n",
    "        training_table: str,\n",
    "        feature_cols: list,\n",
    "        target_col: str,\n",
    "        model_name: str) -> T.Variant:\n",
    "    # 対象データを取得\n",
    "    local_training_data = session.table(training_table).to_pandas()\n",
    "    X = local_training_data[feature_cols]\n",
    "    y = local_training_data[target_col]\n",
    "\n",
    "    # 学習\n",
    "    xgbmodel = XGBRegressor(random_state=123)\n",
    "    xgbmodel.fit(X,y)\n",
    " \n",
    "    # 特徴量重要度を取得\n",
    "    feat_importance = pd.DataFrame(xgbmodel.feature_importances_,feature_cols,columns=['FeatImportance']).to_dict()\n",
    "\n",
    "    # モデルを内部ステージに保存\n",
    "    print(save_file(session, xgbmodel, f'@\"{model_stage_name}\"/{model_name}'))\n",
    "\n",
    "    return feat_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ストアドプロシージャを登録し、実行しましょう。\n",
    "先ほどはデコレータ `@register()` を使用しましたが、ここでは `register()` を使用します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgboost_model_sproc = session.sproc.register(\n",
    "    train_xgboost_model,\n",
    "    stage_location=f'@\"{model_stage_name}\"',\n",
    "    packages=['snowflake-snowpark-python', 'xgboost', 'pandas', 'dill'],\n",
    "    replace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'xgboost_model.sav'\n",
    "\n",
    "feature_cols = df_ml_train.columns\n",
    "target_col = 'AMOUNT_TO_RATIO'\n",
    "feature_cols.remove(target_col)\n",
    "feature_cols.remove('PURCHASE_DATE')\n",
    "\n",
    "feat_importance = train_xgboost_model_sproc(\n",
    "    train_table_name, \n",
    "    feature_cols,\n",
    "    target_col,\n",
    "    model_name, \n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(feat_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論を実行します。\n",
    "UDF を作成して、Snowflake にあるテーブルに対して推論を実行し、結果を Snowflake テーブルに保存します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import sys\n",
    "import cachetools\n",
    "import os\n",
    "import dill\n",
    "from snowflake.snowpark.functions import udf\n",
    "\n",
    "\n",
    "# cachetools というおまじないについては https://docs.snowflake.com/developer-guide/snowpark/reference/python/api/snowflake.snowpark.udf.UDFRegistration.html Example 8 をご参照ください\n",
    "@cachetools.cached(cache={})\n",
    "def load_from_snowflake_import_dir(filename):\n",
    "    '''指定したモデルファイルを読み込む。\n",
    "    対象ファイルがインポートされているか、対象ファイルが存在するステージが stage_location に指定されていることが必要'''\n",
    "    import_dir = sys._xoptions.get('snowflake_import_directory')\n",
    "    with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "        m = dill.load(file)\n",
    "        return m\n",
    "\n",
    "def predict(args: list) -> float:\n",
    "    model = load_from_snowflake_import_dir(model_name)\n",
    "    row = pd.DataFrame(\n",
    "        [args],\n",
    "        columns=feature_cols\n",
    "    )\n",
    "    return model.predict(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測を実行してみましょう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predict_xgboost_regression_udf = session.udf.register(\n",
    "    func=predict,\n",
    "    name='predict_xgboost_regression_udf',\n",
    "    stage_location=f'@\"{model_stage_name}\"',\n",
    "    return_type = T.FloatType(),\n",
    "    replace=True,\n",
    "    is_permanent=True,\n",
    "    imports=[f'@\"{model_stage_name}\"/{model_name}'],\n",
    "    packages=['pandas', 'xgboost', 'cachetools', 'dill'],\n",
    "    session=session\n",
    ")\n",
    "\n",
    "pred_table_name = 'DF_ML_PRED'\n",
    "df_ml_test.select(\n",
    "    F.col('PURCHASE_DATE'),\n",
    "    F.col(target_col),\n",
    "    F.call_udf(\n",
    "        \"predict_xgboost_regression_udf\", \n",
    "        F.array_construct(*feature_cols)).alias(f'PREDICTED_{target_col}'\n",
    "    )\n",
    ").write.mode('overwrite').saveAsTable(pred_table_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測結果を確認してみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_score = session.table(pred_table_name)\n",
    "\n",
    "pd.DataFrame(\n",
    "    pdf_score.collect()\n",
    ").set_index(\n",
    "    'PURCHASE_DATE'\n",
    ").plot(\n",
    "    figsize=(15,5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全然じゃないか（呆れ）\n",
    "\n",
    "雑にモデルを作るとこうなりますが、参加者の皆さまにおかれましては、データをよく見て有用なモデルを作成頂ければと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 提供データを Snowpark で触ってみよう\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほどのデータコラボレーションのお話では、データの組み合わせについて、たくさんのアイデアが紹介されていましたね。\n",
    "\n",
    "ここまでのデモンストレーションをご覧になった皆様なら、もうご自身で、様々なデータ処理ができるのではないでしょうか？\n",
    "\n",
    "最後に、デモで少し扱ったフードデリバリーデータとカレンダーデータを合わせて、データの操作と可視化を行ってみます。  \n",
    "カレンダーだけでも、曜日ごとの傾向や祝日の影響など、さまざまな分析ができそうですね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dely = session.table('MINEDIA_CONTEST.CONTEST.PURCHASE_DELIVERY')\n",
    "df_calender = session.table(\"PODB__JAPANESE_OPEN_DATA_SAMPLE_DATASETS.CALENDAR.E_JAPAN_CALENDAR\")\n",
    "\n",
    "df = df_dely.join(  # カレンダー情報をinner joinで付与\n",
    "    df_calender,\n",
    "    df_dely.PURCHASE_DATE == df_calender.DATE,\n",
    "    join_type = 'inner'\n",
    ").select(\n",
    "    F.col('PURCHASE_DATE'), F.col('PURCHASE_AREA'), F.col('AMOUNT_TO_RATIO'), F.col('WEEKDAY'), F.col('HOLIDAY_NAME')\n",
    ")\n",
    "\n",
    "df.sort(\n",
    "    F.col('AMOUNT_TO_RATIO'),  # 推定購入値の降順で並べ替え\n",
    "    ascending = False\n",
    ").limit(10).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "\n",
    "pdf = pd.DataFrame(\n",
    "    df.group_by(\n",
    "        F.col('WEEKDAY')\n",
    "    ).agg(\n",
    "        F.avg(F.col('AMOUNT_TO_RATIO')).name('AMOUNT_TO_RATIO_AVG')\n",
    "    ).collect()\n",
    ")\n",
    "sns.barplot(x = 'WEEKDAY', y = 'AMOUNT_TO_RATIO_AVG', data = pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "曜日ごとの平均に、そこまで差はないように見受けられます。では、港区に絞ってみるとどうでしょう？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pdf = pd.DataFrame(\n",
    "    df.where(\n",
    "        F.col('PURCHASE_AREA') == '港区'\n",
    "    ).group_by(\n",
    "        F.col('WEEKDAY')\n",
    "    ).agg(\n",
    "        F.avg(F.col('AMOUNT_TO_RATIO')).name('AMOUNT_TO_RATIO_AVG')\n",
    "    ).collect()\n",
    ")\n",
    "sns.barplot(x = 'WEEKDAY', y = 'AMOUNT_TO_RATIO_AVG', data = pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "月曜が低く、水曜が突出しているように見えますね。水曜日に一体何があるのか、もっとデータを眺めてみると、何かわかるかもしれませんね。\n",
    "\n",
    "地域×曜日 の組み合わせ以外にも、様々な組み合わせで集計と可視化を行って、データを深掘りしてみてください！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysnowpark",
   "language": "python",
   "name": "pysnowpark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
